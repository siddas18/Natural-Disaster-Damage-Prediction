{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858b8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "from math import sqrt, isnan, sin, cos, asin, pi, atan2\n",
    "import requests\n",
    "import gzip\n",
    "from functools import reduce\n",
    "from scipy import interpolate\n",
    "\n",
    "import warnings\n",
    "warnings.catch_warnings()\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cf52fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CSV files...\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "def get_NOAA_data():\n",
    "    \n",
    "    URL = \"https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/\"\n",
    "    r = requests.get(URL)\n",
    "    file_names = pd.read_html(r.text)[0]['Name']\n",
    "    events_file_names = file_names[file_names.str.contains(\"details\",na=False)]\n",
    "    noaa_list = []\n",
    "    \n",
    "    print(\"Extracting CSV files...\")\n",
    "    for file in events_file_names:\n",
    "        full_URL = URL + file\n",
    "        with gzip.open(requests.get(full_URL, stream=True).raw) as f:\n",
    "            noaa_list.append(pd.read_csv(f))\n",
    "        \n",
    "    df = pd.concat(noaa_list)\n",
    "    \n",
    "    print(\"Completed\")\n",
    "    return df\n",
    "\n",
    "def pickle_source_data():\n",
    "    noaa_source_df = get_NOAA_data()\n",
    "    home_dir = os.getcwd()\n",
    "    data_dir = os.path.join(home_dir, \"Data\")\n",
    "    try:\n",
    "        os.mkdir(data_dir)\n",
    "        os.chdir(data_dir)\n",
    "    except OSError:\n",
    "        os.chdir(data_dir)\n",
    "        for file in os.listdir():\n",
    "            os.remove(file)\n",
    "    noaa_source_df.to_pickle('noaa_source_data.pkl')\n",
    "    os.chdir(home_dir)\n",
    "    return noaa_source_df\n",
    "\n",
    "NOAA_df = pickle_source_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718d5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA_df = pd.read_pickle('Data/noaa_source_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d05e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_str2num(x):\n",
    "    if type(x) == float or type(x) == int:\n",
    "        return float(x)\n",
    "    num = 1 if x[:-1] == '' else x[:-1]        \n",
    "    if x[-1] == 'T':\n",
    "        return float(num) * 1000000000000\n",
    "    elif x[-1] == 'B':\n",
    "        return float(num) * 1000000000\n",
    "    elif x[-1] == 'M':\n",
    "        return float(num) * 1000000\n",
    "    elif x[-1] == 'K' or x[-1] == 'k':\n",
    "        return float(num) * 1000\n",
    "    elif x[-1] == 'h' or x[-1] == 'H':\n",
    "        return float(num) * 100\n",
    "    elif x[-1] == '?':\n",
    "        return float(num)\n",
    "    else:\n",
    "        return float(x)\n",
    "\n",
    "def winds(x):\n",
    "    if x['MAGNITUDE_TYPE'] in ['EG', 'E', 'M', 'ES', 'MG', 'MS']:\n",
    "        return x['MAGNITUDE']\n",
    "\n",
    "def hail(x):\n",
    "    if x['MAGNITUDE_TYPE'] not in ['EG', 'E', 'M', 'ES', 'MG', 'MS']:\n",
    "        return x['MAGNITUDE']\n",
    "\n",
    "def missing_swap(df, col1, col2):\n",
    "    df.loc[~df[col1].isnull() & df[col2].isnull(), col2] = df.loc[~df[col1].isnull() & df[col2].isnull(), col1]\n",
    "    df.loc[df[col1].isnull() & ~df[col2].isnull(), col1] = df.loc[df[col1].isnull() & ~df[col2].isnull(), col2]\n",
    "    return df\n",
    "\n",
    "def calc_duration(x):\n",
    "    begin_dt = dt.strptime(x['BEGIN_DATE_TIME'], \"%d-%b-%y %H:%M:%S\")\n",
    "    end_dt = dt.strptime(x['END_DATE_TIME'], \"%d-%b-%y %H:%M:%S\")\n",
    "    difference = end_dt - begin_dt\n",
    "    difference_days = difference.days + difference.seconds/86400\n",
    "    return difference_days\n",
    "\n",
    "def geo_distance(x):\n",
    "    # Source : https://en.wikipedia.org/wiki/Haversine_formula\n",
    "    p = pi/180\n",
    "    lat1 = x['BEGIN_LAT']\n",
    "    lat2 = x['END_LAT']\n",
    "    lon1 = x['BEGIN_LON']\n",
    "    lon2 = x['END_LON']\n",
    "    R = 6371\n",
    "    dLat = p * (lat2-lat1)\n",
    "    dLon = p * (lon2-lon1)\n",
    "    a = sin(dLat/2) ** 2 + cos(p*lat1) * cos(p*lat2) * sin(dLon/2) ** 2\n",
    "    return 2 * R * atan2(sqrt(a), sqrt(1-a))\n",
    "\n",
    "rename_event_dict = {\n",
    "    'TORNADOES, TSTM WIND, HAIL': 'Tornadoes, Thunderstorm Wind, Hail',\n",
    "    'THUNDERSTORM WINDS LIGHTNING': 'Thunderstorm Wind, Lightning',\n",
    "    'THUNDERSTORM WINDS/ FLOOD': 'Thunderstorm Wind, Flood',\n",
    "    'THUNDERSTORM WINDS/FLOODING': 'Thunderstorm Wind, Flood',\n",
    "    'THUNDERSTORM WIND/ TREES': 'Thunderstorm Wind, Trees',\n",
    "    'THUNDERSTORM WIND/ TREE': 'Thunderstorm Wind, Trees',\n",
    "    'THUNDERSTORM WINDS/HEAVY RAIN': 'Thunderstorm Wind, Heavy Rain',\n",
    "    'TORNADO/WATERSPOUT': 'Tornado, Waterspout',\n",
    "    'THUNDERSTORM WINDS FUNNEL CLOU': 'Thunderstorm Wind, Funnel Cloud',\n",
    "    'THUNDERSTORM WINDS/FLASH FLOOD': 'Thunderstorm Wind, Flash Flood',\n",
    "    'HAIL/ICY ROADS': 'Hail/ Icy Roads',\n",
    "    'HAIL FLOODING': 'Hail, Flood',\n",
    "    'THUNDERSTORM WINDS HEAVY RAIN': 'Thunderstorm Wind, Heavy Rain',\n",
    "    'Hurricane (Typhoon)': 'Hurricane'\n",
    "}\n",
    "\n",
    "timezone_dict = {\n",
    "    'GST' : ['GST10'],\n",
    "    'AST' : ['AST-4', 'AST'],\n",
    "    'EST' : ['EST', 'EST-5', 'ESt', 'EDT'],\n",
    "    'CST' : ['CST', 'CST-6', 'CSt', 'CSC', 'SCT', 'GMT', 'UNK', 'CDT'],\n",
    "    'MST' : ['MST', 'MST-7', 'MDT'],\n",
    "    'PST' : ['PST', 'PST-8', 'PDT'],\n",
    "    'AKST': ['AKST-9'],\n",
    "    'HST' : ['HST-10', 'HST'],\n",
    "    'SST' : ['SST-11', 'SST']\n",
    "}\n",
    "\n",
    "azimuth_mapping ={ 'N/A': ['ND', 'EE', 'TO', 'MI', 'M', 'EST', 'EAS', 'TH', 'WES'] }\n",
    "\n",
    "def dict_mapping(x):\n",
    "    for key, val in timezone_dict.items():\n",
    "        if x in val:\n",
    "            return key\n",
    "        \n",
    "def tor_scale(x):\n",
    "    if type(x)==float or x[-1] == 'U':\n",
    "        return 0\n",
    "    else:\n",
    "        return int(x[-1]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d90b29aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>...</th>\n",
       "      <th>END_LAT</th>\n",
       "      <th>END_LON</th>\n",
       "      <th>DATA_SOURCE</th>\n",
       "      <th>WIND_SPEED</th>\n",
       "      <th>HAIL_SIZE</th>\n",
       "      <th>DURATION</th>\n",
       "      <th>GEO_DISTANCE</th>\n",
       "      <th>COLD_WEATHER_EVENT</th>\n",
       "      <th>WINDY_EVENT</th>\n",
       "      <th>WATER_EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195004</td>\n",
       "      <td>28</td>\n",
       "      <td>1445</td>\n",
       "      <td>195004</td>\n",
       "      <td>28</td>\n",
       "      <td>1445</td>\n",
       "      <td>10096222</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>35.17</td>\n",
       "      <td>-99.20</td>\n",
       "      <td>PUB</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.559746</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195004</td>\n",
       "      <td>29</td>\n",
       "      <td>1530</td>\n",
       "      <td>195004</td>\n",
       "      <td>29</td>\n",
       "      <td>1530</td>\n",
       "      <td>10120412</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>31.73</td>\n",
       "      <td>-98.60</td>\n",
       "      <td>PUB</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.903138</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1800</td>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1800</td>\n",
       "      <td>10104927</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>40.65</td>\n",
       "      <td>-75.47</td>\n",
       "      <td>PUB</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.916105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1830</td>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1830</td>\n",
       "      <td>10104928</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>40.60</td>\n",
       "      <td>-76.75</td>\n",
       "      <td>PUB</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195007</td>\n",
       "      <td>24</td>\n",
       "      <td>1440</td>\n",
       "      <td>195007</td>\n",
       "      <td>24</td>\n",
       "      <td>1440</td>\n",
       "      <td>10104929</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>41.63</td>\n",
       "      <td>-79.68</td>\n",
       "      <td>PUB</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "0           195004         28        1445         195004       28      1445   \n",
       "1           195004         29        1530         195004       29      1530   \n",
       "2           195007          5        1800         195007        5      1800   \n",
       "3           195007          5        1830         195007        5      1830   \n",
       "4           195007         24        1440         195007       24      1440   \n",
       "\n",
       "   EVENT_ID         STATE STATE_FIPS  YEAR  ... END_LAT END_LON DATA_SOURCE  \\\n",
       "0  10096222      OKLAHOMA       40.0  1950  ...   35.17  -99.20         PUB   \n",
       "1  10120412         TEXAS       48.0  1950  ...   31.73  -98.60         PUB   \n",
       "2  10104927  PENNSYLVANIA       42.0  1950  ...   40.65  -75.47         PUB   \n",
       "3  10104928  PENNSYLVANIA       42.0  1950  ...   40.60  -76.75         PUB   \n",
       "4  10104929  PENNSYLVANIA       42.0  1950  ...   41.63  -79.68         PUB   \n",
       "\n",
       "   WIND_SPEED HAIL_SIZE DURATION GEO_DISTANCE COLD_WEATHER_EVENT WINDY_EVENT  \\\n",
       "0        59.0       0.0      0.0     5.559746                  0           1   \n",
       "1        59.0       0.0      0.0    18.903138                  0           1   \n",
       "2        59.0       0.0      0.0    20.916105                  0           1   \n",
       "3        59.0       0.0      0.0     0.000000                  0           1   \n",
       "4        59.0       0.0      0.0     0.000000                  0           1   \n",
       "\n",
       "   WATER_EVENT  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def impute_NOAA_data(df):\n",
    "    \n",
    "    # maginitude converted into other variables\n",
    "    drop_list = ['EVENT_NARRATIVE', 'EPISODE_NARRATIVE', 'EPISODE_ID', 'MAGNITUDE']\n",
    "    impute_mean_list = ['BEGIN_LAT', 'END_LAT', 'BEGIN_LON', 'END_LON', 'DAMAGE_CROPS', 'DURATION', 'GEO_DISTANCE']\n",
    "    impute_event_mean_list = ['DAMAGE_CROPS', 'DURATION', 'GEO_DISTANCE', 'TOR_WIDTH', 'TOR_LENGTH', 'BEGIN_RANGE', \\\n",
    "                              'END_RANGE', 'WIND_SPEED', 'HAIL_SIZE']\n",
    "    \n",
    "    # imputing damage columns with 0 for the time-being\n",
    "    impute_zero_list = ['TOR_WIDTH', 'TOR_LENGTH', 'BEGIN_RANGE', 'END_RANGE', 'WIND_SPEED', 'HAIL_SIZE']\n",
    "    impute_NA_list = ['TOR_OTHER_CZ_NAME', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_WFO', \\\n",
    "                      'CATEGORY', 'FLOOD_CAUSE', 'SOURCE', 'WFO', 'CZ_NAME', 'DATA_SOURCE', 'STATE', \\\n",
    "                      'STATE_FIPS', 'MAGNITUDE_TYPE', 'BEGIN_AZIMUTH', 'END_AZIMUTH', 'BEGIN_LOCATION', \\\n",
    "                      'END_LOCATION', 'TOR_OTHER_CZ_FIPS']\n",
    "    \n",
    "    # Remove rows with no damage info\n",
    "    df.dropna(subset=['DAMAGE_PROPERTY'], axis=0, inplace=True)\n",
    "    \n",
    "    # Removing inconsistencies\n",
    "    df = missing_swap(df, 'BEGIN_RANGE', 'END_RANGE')\n",
    "    df = missing_swap(df, 'BEGIN_LAT', 'END_LAT')\n",
    "    df = missing_swap(df, 'BEGIN_LON', 'END_LON')\n",
    "    df = missing_swap(df, 'BEGIN_AZIMUTH', 'END_AZIMUTH')\n",
    "    df = missing_swap(df, 'BEGIN_LOCATION', 'END_LOCATION')\n",
    "    \n",
    "    # Damage variables cleaning\n",
    "    df['DAMAGE_PROPERTY'] = df.DAMAGE_PROPERTY.map(replace_str2num)\n",
    "    df['DAMAGE_CROPS'] = df.DAMAGE_CROPS.map(replace_str2num)\n",
    "    \n",
    "    # Splitting magnitude variable into constituent attributes\n",
    "    df['WIND_SPEED'] = df.apply(winds, axis = 1)\n",
    "    df['HAIL_SIZE'] = df.apply(hail, axis = 1)\n",
    "    \n",
    "    # Feature cleaning and engineering\n",
    "    df['DURATION'] = df[['BEGIN_DATE_TIME', 'END_DATE_TIME']].apply(lambda x: calc_duration(x), axis=1)\n",
    "    df['GEO_DISTANCE'] = df.apply(lambda x: geo_distance(x), axis=1)\n",
    "    df['EVENT_TYPE'] = df['EVENT_TYPE'].apply(lambda x: rename_event_dict[x] if rename_event_dict.get(x)!= None else x)\n",
    "    df['COLD_WEATHER_EVENT'] = df['EVENT_TYPE'].str.contains('Hail|Winter|Snow|Chill|Cold|Frost|Freeze|Blizzard|Ice|Avalanche').map({True: 1, False:0})\n",
    "    df['WINDY_EVENT'] = df['EVENT_TYPE'].str.contains('Wind|Tornado|Thunderstorm|Cloud|Storm').map({True: 1, False:0})\n",
    "    df['WATER_EVENT'] = df['EVENT_TYPE'].str.contains('Flood|Marine|Rain|Hurricane|Tide|Lake|Seiche|Tsunami|Sleet|Water').map({True: 1, False:0})\n",
    "    df.loc[:,'CZ_TIMEZONE'] = df.loc[:,'CZ_TIMEZONE'].apply(lambda x: dict_mapping(x))\n",
    "    df.loc[:,'BEGIN_AZIMUTH'] = df.loc[:,'BEGIN_AZIMUTH'].str.upper().apply(lambda x: dict_mapping(x) if dict_mapping(x) != None else x)\n",
    "    df.loc[:,'END_AZIMUTH'] = df.loc[:,'END_AZIMUTH'].str.upper().apply(lambda x: dict_mapping(x) if dict_mapping(x) != None else x)\n",
    "    df.loc[:,'TOR_F_SCALE'] = df.loc[:,'TOR_F_SCALE'].str.upper().apply(lambda x: tor_scale(x))\n",
    "    \n",
    "    # Imputing few columns with mean value for each event type\n",
    "    for col in impute_event_mean_list:\n",
    "        df[col] = df.groupby(\"EVENT_TYPE\")[col].transform(lambda x: x.fillna(x.mean()))\n",
    "    \n",
    "    # Imputing string columns with missing values with N/A\n",
    "    for col in impute_NA_list:\n",
    "        df[col] = df[col].astype('str').apply(lambda x: 'N/A' if x=='nan' else x)\n",
    "        \n",
    "    # Imputing float columns having missing values with 0.0\n",
    "    for col in impute_zero_list:\n",
    "        df[col] = df[col].fillna(0.0)\n",
    "        \n",
    "    # Imputing latitude and longitudes with average value\n",
    "    for col in impute_mean_list:\n",
    "        df[impute_mean_list] = df[impute_mean_list].transform(lambda x: x.fillna(x.mean()))\n",
    "        \n",
    "    # Dropping text and ID columns\n",
    "    for col in drop_list:\n",
    "        df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "imputed_NOAA_df = impute_NOAA_data(NOAA_df.copy())\n",
    "imputed_NOAA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e005724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EPA_data():\n",
    "    \n",
    "    ground_temp_df = pd.read_csv('https://www.epa.gov/sites/default/files/2021-04/temperature_fig-1.csv', skiprows=6, usecols = [0, 1], encoding='latin1')\n",
    "    ground_temp_df.columns = [\"YEAR\", \"SURFACE_TEMP_DIFF\"]\n",
    "    \n",
    "    greenhouse_df = pd.read_csv(\"https://www.epa.gov/sites/default/files/2021-04/us-ghg-emissions_fig-1.csv\",skiprows=6)\n",
    "    greenhouse_df.columns = [\"YEAR\", \"CO2\", \"METHANE\", \"N2O\", \"HFC, PFC, SF6, NF3\"]\n",
    "    \n",
    "    precipitation_df =  pd.read_csv('https://www.epa.gov/sites/default/files/2021-04/heavy-precip_fig-1.csv',skiprows=6)\n",
    "    precipitation_df.columns = [\"YEAR\", \"PPT_LAND_AREA\", \"PPT_NINE_YEAR_AVG\"]\n",
    "    \n",
    "    sea_level_df = pd.read_csv('https://www.epa.gov/sites/default/files/2021-04/sea-level_fig-1.csv', skiprows=6)\n",
    "    sea_level_df.columns = [\"YEAR\", \"CSIRO_ADJ_SEA_LEVEL\", \"CSIRO_LOWER\", \"CSIRO_UPPER\", \"NOAA_ADJ_SEA_LEVEL\"]\n",
    "    sea_level_df.loc[sea_level_df['YEAR'] > 1992, 'CSIRO_ADJ_SEA_LEVEL'] = sea_level_df.loc[sea_level_df['YEAR'] > 1992, 'NOAA_ADJ_SEA_LEVEL']\n",
    "    sea_level_df.drop(['CSIRO_LOWER', 'CSIRO_UPPER', 'NOAA_ADJ_SEA_LEVEL'], axis=1, inplace=True)\n",
    "    \n",
    "    seasonal_temp_df = pd.read_csv('https://www.epa.gov/sites/default/files/2021-04/seasonal-temperature_fig-1.csv', skiprows=6)\n",
    "    seasonal_temp_df.columns = [\"YEAR\", \"WINTER_ANOMALY\", \"SPRING_ANOMALY\", \"SUMMER_ANOMALY\", \"FALL_ANOMALY\"]\n",
    "    \n",
    "    arctic_ice_df = pd.read_csv('https://www.epa.gov/sites/default/files/2021-03/arctic-sea-ice_fig-1.csv', skiprows=6)\n",
    "    arctic_ice_df.columns = [\"YEAR\", \"ICE_CVG_MARCH\", \"ICE_CVG_SEP\"]\n",
    "    \n",
    "    glacier_df = pd.read_csv('https://www.epa.gov/sites/default/files/2021-03/glaciers_fig-1.csv', skiprows=6)\n",
    "    glacier_df.columns = [\"YEAR\", \"GLACIER_MASS_BAL\", \"NUM_OBS\"]\n",
    "    glacier_df.drop(['NUM_OBS'], axis=1, inplace=True)\n",
    "    \n",
    "    dfs = [ground_temp_df, greenhouse_df, precipitation_df, sea_level_df, seasonal_temp_df, arctic_ice_df, glacier_df]\n",
    "    epa_df = reduce(lambda left, right: pd.merge(left, right, how=\"outer\", on=\"YEAR\"), dfs)\n",
    "    epa_df = epa_df[epa_df.YEAR >= 1950]\n",
    "    \n",
    "    return epa_df\n",
    "\n",
    "epa_source_df = get_EPA_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0262bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>SURFACE_TEMP_DIFF</th>\n",
       "      <th>CO2</th>\n",
       "      <th>METHANE</th>\n",
       "      <th>N2O</th>\n",
       "      <th>HFC, PFC, SF6, NF3</th>\n",
       "      <th>PPT_LAND_AREA</th>\n",
       "      <th>PPT_NINE_YEAR_AVG</th>\n",
       "      <th>CSIRO_ADJ_SEA_LEVEL</th>\n",
       "      <th>WINTER_ANOMALY</th>\n",
       "      <th>SPRING_ANOMALY</th>\n",
       "      <th>SUMMER_ANOMALY</th>\n",
       "      <th>FALL_ANOMALY</th>\n",
       "      <th>ICE_CVG_MARCH</th>\n",
       "      <th>ICE_CVG_SEP</th>\n",
       "      <th>GLACIER_MASS_BAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1950</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>2659.760945</td>\n",
       "      <td>1192.496732</td>\n",
       "      <td>473.537757</td>\n",
       "      <td>161.828535</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.085488</td>\n",
       "      <td>3.598425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6.286129</td>\n",
       "      <td>3.648314</td>\n",
       "      <td>2.472857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1951</td>\n",
       "      <td>-0.90</td>\n",
       "      <td>2721.103288</td>\n",
       "      <td>1182.687741</td>\n",
       "      <td>473.161684</td>\n",
       "      <td>160.274465</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.093453</td>\n",
       "      <td>3.972441</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>-0.66</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>6.278021</td>\n",
       "      <td>3.616373</td>\n",
       "      <td>2.060714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1952</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2782.445631</td>\n",
       "      <td>1172.878751</td>\n",
       "      <td>472.785612</td>\n",
       "      <td>158.720395</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.099285</td>\n",
       "      <td>3.870079</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.16</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>6.269913</td>\n",
       "      <td>3.584432</td>\n",
       "      <td>1.648571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1953</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2843.787975</td>\n",
       "      <td>1163.069761</td>\n",
       "      <td>472.409540</td>\n",
       "      <td>157.166325</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.102855</td>\n",
       "      <td>4.043307</td>\n",
       "      <td>3.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.87</td>\n",
       "      <td>6.261805</td>\n",
       "      <td>3.552491</td>\n",
       "      <td>1.236429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1954</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2905.130318</td>\n",
       "      <td>1153.260771</td>\n",
       "      <td>472.033467</td>\n",
       "      <td>155.612254</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.101262</td>\n",
       "      <td>3.929134</td>\n",
       "      <td>3.10</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.51</td>\n",
       "      <td>6.253697</td>\n",
       "      <td>3.520550</td>\n",
       "      <td>0.824286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    YEAR  SURFACE_TEMP_DIFF          CO2      METHANE         N2O  \\\n",
       "49  1950              -0.63  2659.760945  1192.496732  473.537757   \n",
       "50  1951              -0.90  2721.103288  1182.687741  473.161684   \n",
       "51  1952               0.25  2782.445631  1172.878751  472.785612   \n",
       "52  1953               1.35  2843.787975  1163.069761  472.409540   \n",
       "53  1954               1.31  2905.130318  1153.260771  472.033467   \n",
       "\n",
       "    HFC, PFC, SF6, NF3  PPT_LAND_AREA  PPT_NINE_YEAR_AVG  CSIRO_ADJ_SEA_LEVEL  \\\n",
       "49          161.828535          0.100           0.085488             3.598425   \n",
       "50          160.274465          0.107           0.093453             3.972441   \n",
       "51          158.720395          0.077           0.099285             3.870079   \n",
       "52          157.166325          0.123           0.102855             4.043307   \n",
       "53          155.612254          0.099           0.101262             3.929134   \n",
       "\n",
       "    WINTER_ANOMALY  SPRING_ANOMALY  SUMMER_ANOMALY  FALL_ANOMALY  \\\n",
       "49            0.85           -1.94           -1.68          0.32   \n",
       "50            0.35           -1.24           -0.66         -1.49   \n",
       "51            0.67           -0.94            1.16         -0.63   \n",
       "52            3.02           -0.36            0.75          1.87   \n",
       "53            3.10           -0.25            0.77          1.51   \n",
       "\n",
       "    ICE_CVG_MARCH  ICE_CVG_SEP  GLACIER_MASS_BAL  \n",
       "49       6.286129     3.648314          2.472857  \n",
       "50       6.278021     3.616373          2.060714  \n",
       "51       6.269913     3.584432          1.648571  \n",
       "52       6.261805     3.552491          1.236429  \n",
       "53       6.253697     3.520550          0.824286  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def impute_EPA_DATA(df, breaks):\n",
    "    fillable_cols = df.columns[df.isnull().sum() > 0]\n",
    "    for col in fillable_cols:\n",
    "        temp_df = df[['YEAR', col]]\n",
    "        present_df = temp_df[~ temp_df[col].isnull()]\n",
    "        null_df = temp_df[temp_df[col].isnull()]\n",
    "        years = sorted(np.random.choice(present_df['YEAR'], breaks))\n",
    "        input_df = present_df[present_df['YEAR'].isin(years)]\n",
    "        func = interpolate.interp1d(input_df['YEAR'], input_df[col], fill_value=\"extrapolate\")\n",
    "        temp_df['INTERPOLATION'] = func(temp_df['YEAR'])\n",
    "        df[col] = temp_df.apply(lambda x: x['INTERPOLATION'] if isnan(x[col]) else x[col], axis=1)\n",
    "    return df\n",
    "        \n",
    "imputed_EPA_df = impute_EPA_DATA(epa_source_df.copy(), 6)\n",
    "imputed_EPA_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb7e4bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BEGIN_YEARMONTH</th>\n",
       "      <th>BEGIN_DAY</th>\n",
       "      <th>BEGIN_TIME</th>\n",
       "      <th>END_YEARMONTH</th>\n",
       "      <th>END_DAY</th>\n",
       "      <th>END_TIME</th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>STATE_FIPS</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>...</th>\n",
       "      <th>PPT_LAND_AREA</th>\n",
       "      <th>PPT_NINE_YEAR_AVG</th>\n",
       "      <th>CSIRO_ADJ_SEA_LEVEL</th>\n",
       "      <th>WINTER_ANOMALY</th>\n",
       "      <th>SPRING_ANOMALY</th>\n",
       "      <th>SUMMER_ANOMALY</th>\n",
       "      <th>FALL_ANOMALY</th>\n",
       "      <th>ICE_CVG_MARCH</th>\n",
       "      <th>ICE_CVG_SEP</th>\n",
       "      <th>GLACIER_MASS_BAL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>195004</td>\n",
       "      <td>28</td>\n",
       "      <td>1445</td>\n",
       "      <td>195004</td>\n",
       "      <td>28</td>\n",
       "      <td>1445</td>\n",
       "      <td>10096222</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.085488</td>\n",
       "      <td>3.598425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6.286129</td>\n",
       "      <td>3.648314</td>\n",
       "      <td>2.472857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195004</td>\n",
       "      <td>29</td>\n",
       "      <td>1530</td>\n",
       "      <td>195004</td>\n",
       "      <td>29</td>\n",
       "      <td>1530</td>\n",
       "      <td>10120412</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.085488</td>\n",
       "      <td>3.598425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6.286129</td>\n",
       "      <td>3.648314</td>\n",
       "      <td>2.472857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1800</td>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1800</td>\n",
       "      <td>10104927</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.085488</td>\n",
       "      <td>3.598425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6.286129</td>\n",
       "      <td>3.648314</td>\n",
       "      <td>2.472857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1830</td>\n",
       "      <td>195007</td>\n",
       "      <td>5</td>\n",
       "      <td>1830</td>\n",
       "      <td>10104928</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.085488</td>\n",
       "      <td>3.598425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6.286129</td>\n",
       "      <td>3.648314</td>\n",
       "      <td>2.472857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195007</td>\n",
       "      <td>24</td>\n",
       "      <td>1440</td>\n",
       "      <td>195007</td>\n",
       "      <td>24</td>\n",
       "      <td>1440</td>\n",
       "      <td>10104929</td>\n",
       "      <td>PENNSYLVANIA</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.085488</td>\n",
       "      <td>3.598425</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-1.94</td>\n",
       "      <td>-1.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>6.286129</td>\n",
       "      <td>3.648314</td>\n",
       "      <td>2.472857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BEGIN_YEARMONTH  BEGIN_DAY  BEGIN_TIME  END_YEARMONTH  END_DAY  END_TIME  \\\n",
       "0           195004         28        1445         195004       28      1445   \n",
       "1           195004         29        1530         195004       29      1530   \n",
       "2           195007          5        1800         195007        5      1800   \n",
       "3           195007          5        1830         195007        5      1830   \n",
       "4           195007         24        1440         195007       24      1440   \n",
       "\n",
       "   EVENT_ID         STATE STATE_FIPS  YEAR  ... PPT_LAND_AREA  \\\n",
       "0  10096222      OKLAHOMA       40.0  1950  ...           0.1   \n",
       "1  10120412         TEXAS       48.0  1950  ...           0.1   \n",
       "2  10104927  PENNSYLVANIA       42.0  1950  ...           0.1   \n",
       "3  10104928  PENNSYLVANIA       42.0  1950  ...           0.1   \n",
       "4  10104929  PENNSYLVANIA       42.0  1950  ...           0.1   \n",
       "\n",
       "  PPT_NINE_YEAR_AVG CSIRO_ADJ_SEA_LEVEL  WINTER_ANOMALY SPRING_ANOMALY  \\\n",
       "0          0.085488            3.598425            0.85          -1.94   \n",
       "1          0.085488            3.598425            0.85          -1.94   \n",
       "2          0.085488            3.598425            0.85          -1.94   \n",
       "3          0.085488            3.598425            0.85          -1.94   \n",
       "4          0.085488            3.598425            0.85          -1.94   \n",
       "\n",
       "  SUMMER_ANOMALY FALL_ANOMALY ICE_CVG_MARCH ICE_CVG_SEP  GLACIER_MASS_BAL  \n",
       "0          -1.68         0.32      6.286129    3.648314          2.472857  \n",
       "1          -1.68         0.32      6.286129    3.648314          2.472857  \n",
       "2          -1.68         0.32      6.286129    3.648314          2.472857  \n",
       "3          -1.68         0.32      6.286129    3.648314          2.472857  \n",
       "4          -1.68         0.32      6.286129    3.648314          2.472857  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df = pd.merge(imputed_NOAA_df, imputed_EPA_df, on='YEAR', how='inner')\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99f9285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_pickle('Data/cleaned_data.pkl', protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
